# H∆∞·ªõng d·∫´n Inference Model TTS Ti·∫øng Vi·ªát

## T·ªïng quan
Sau khi train xong model, b·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng c√°c script inference ƒë·ªÉ t·∫°o audio t·ª´ text ti·∫øng Vi·ªát.

## Files c·∫ßn thi·∫øt

### 1. Model ƒë√£ train
```
vietnamese_model_output/
‚îú‚îÄ‚îÄ final_vietnamese_model.pt          # Model cu·ªëi c√πng
‚îú‚îÄ‚îÄ checkpoint_epoch_10.pt             # Checkpoint epoch 10
‚îú‚îÄ‚îÄ checkpoint_epoch_9.pt              # Checkpoint epoch 9
‚îî‚îÄ‚îÄ ...
```

### 2. Tokenizer Vietnamese
```
tokenizer_vi_expanded.json              # Tokenizer ƒë√£ merge (1997 tokens)
```

## C√°ch s·ª≠ d·ª•ng

### Option 1: Script ƒë∆°n gi·∫£n (Recommended)

```bash
cd chatterbox
python inference_simple.py
```

**Features:**
- ‚úÖ D·ªÖ s·ª≠ d·ª•ng
- ‚úÖ Demo v·ªõi test texts c√≥ s·∫µn
- ‚úÖ Interactive mode
- ‚úÖ T·ª± ƒë·ªông t·∫°o th∆∞ m·ª•c output

### Option 2: Script v·ªõi model architecture ƒë·∫ßy ƒë·ªß

```bash
cd chatterbox  
python inference_with_model.py
```

**Features:**
- ‚úÖ S·ª≠ d·ª•ng ch√≠nh x√°c model architecture ƒë√£ train
- ‚úÖ Load checkpoint v·ªõi th√¥ng tin training
- ‚úÖ Batch generation
- ‚úÖ Interactive mode

### Option 3: Script inference n√¢ng cao

```bash
cd chatterbox
python inference_vietnamese_tts.py --model vietnamese_model_output/final_vietnamese_model.pt --tokenizer tokenizer_vi_expanded.json --text "Xin ch√†o c√°c b·∫°n"
```

**Command line options:**
```bash
# Single text
python inference_vietnamese_tts.py \
    --model vietnamese_model_output/final_vietnamese_model.pt \
    --tokenizer tokenizer_vi_expanded.json \
    --text "Xin ch√†o, t√¥i l√† AI ti·∫øng Vi·ªát" \
    --output hello_vietnamese.wav

# Batch t·ª´ file
echo "Xin ch√†o c√°c b·∫°n" > texts.txt
echo "H√¥m nay tr·ªùi ƒë·∫πp" >> texts.txt
python inference_vietnamese_tts.py \
    --model vietnamese_model_output/final_vietnamese_model.pt \
    --tokenizer tokenizer_vi_expanded.json \
    --text_file texts.txt \
    --output_dir batch_output

# Interactive mode
python inference_vietnamese_tts.py \
    --model vietnamese_model_output/final_vietnamese_model.pt \
    --tokenizer tokenizer_vi_expanded.json
```

## K·∫øt qu·∫£ mong ƒë·ª£i

### Output files
```
inference_output/
‚îú‚îÄ‚îÄ vietnamese_tts_001_20241206_143022.wav
‚îú‚îÄ‚îÄ vietnamese_tts_002_20241206_143023.wav
‚îú‚îÄ‚îÄ interactive_20241206_143045.wav
‚îî‚îÄ‚îÄ ...
```

### Console output
```
‚úÖ Vietnamese TTS Model loaded on cuda
üìù Vocabulary size: 1997
üéØ Processing text: 'Xin ch√†o c√°c b·∫°n'
üìù Text tokens shape: torch.Size([1, 17])
üéµ Mel output shape: torch.Size([1, 68, 80])
üíæ Audio saved: inference_output/vietnamese_tts_001_20241206_143022.wav
```

## L∆∞u √Ω quan tr·ªçng

### ‚ö†Ô∏è Vocoder Limitation
Hi·ªán t·∫°i scripts s·ª≠ d·ª•ng **placeholder vocoder** (t·∫°o audio ng·∫´u nhi√™n) v√¨:
- Model ch·ªâ t·∫°o mel spectrogram
- C·∫ßn vocoder ri√™ng ƒë·ªÉ convert mel ‚Üí audio
- Vocoder th·ª±c s·ª± c·∫ßn train ri√™ng ho·∫∑c d√πng pre-trained

### üîß ƒê·ªÉ c√≥ audio th·ª±c s·ª±, c·∫ßn:

1. **S·ª≠ d·ª•ng pre-trained vocoder:**
```python
# V√≠ d·ª• v·ªõi HiFi-GAN
from hifigan import HiFiGAN
vocoder = HiFiGAN.from_pretrained("hifigan-universal")
audio = vocoder(mel_spectrogram)
```

2. **Ho·∫∑c train vocoder ri√™ng:**
```bash
# Train HiFi-GAN v·ªõi d·ªØ li·ªáu ti·∫øng Vi·ªát
python train_hifigan.py --data vietnamese_audio_data
```

## Troubleshooting

### 1. Model kh√¥ng load ƒë∆∞·ª£c
```bash
# Ki·ªÉm tra file t·ªìn t·∫°i
ls -la vietnamese_model_output/
ls -la tokenizer_vi_expanded.json

# Th·ª≠ v·ªõi checkpoint kh√°c
python inference_with_model.py  # S·∫Ω list available models
```

### 2. CUDA out of memory
```python
# Trong script, thay ƒë·ªïi device
tts = VietnameseTTSInference(model_path, tokenizer_path, device='cpu')
```

### 3. Tokenizer error
```bash
# Ki·ªÉm tra tokenizer format
python -c "
import json
with open('tokenizer_vi_expanded.json', 'r') as f:
    data = json.load(f)
print('Vocab size:', len(data['model']['vocab']))
"
```

## T·ªëi ∆∞u h√≥a

### 1. TƒÉng t·ªëc inference
```python
# S·ª≠ d·ª•ng torch.jit.script
model = torch.jit.script(model)

# Ho·∫∑c torch.compile (PyTorch 2.0+)
model = torch.compile(model)
```

### 2. Batch processing
```python
# Process nhi·ªÅu texts c√πng l√∫c
texts = ["Text 1", "Text 2", "Text 3"]
results = tts.batch_generate(texts)
```

### 3. Memory optimization
```python
# Clear cache sau m·ªói inference
torch.cuda.empty_cache()
```

## Next Steps

### 1. C·∫£i thi·ªán ch·∫•t l∆∞·ª£ng audio
- Train vocoder ri√™ng cho ti·∫øng Vi·ªát
- Fine-tune v·ªõi d·ªØ li·ªáu ch·∫•t l∆∞·ª£ng cao
- S·ª≠ d·ª•ng advanced vocoder (HiFi-GAN, WaveGlow)

### 2. T·ªëi ∆∞u model
- Model quantization
- ONNX export cho deployment
- TensorRT optimization

### 3. Production deployment
- API server v·ªõi FastAPI
- Docker containerization
- Cloud deployment (AWS, GCP, Azure)

## V√≠ d·ª• s·ª≠ d·ª•ng trong code

```python
from inference_with_model import VietnameseTTSInference

# Kh·ªüi t·∫°o
tts = VietnameseTTSInference(
    model_path="vietnamese_model_output/final_vietnamese_model.pt",
    tokenizer_path="tokenizer_vi_expanded.json"
)

# T·∫°o audio
audio = tts.text_to_speech(
    text="Xin ch√†o, t√¥i l√† tr·ª£ l√Ω AI ti·∫øng Vi·ªát",
    output_path="output.wav"
)

# Batch processing
texts = ["C√¢u 1", "C√¢u 2", "C√¢u 3"]
results = tts.batch_generate(texts, output_dir="batch_output")
```
