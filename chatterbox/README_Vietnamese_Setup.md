# Vietnamese TTS Training - Complete Setup Guide

HÆ°á»›ng dáº«n hoÃ n chá»‰nh Ä‘á»ƒ setup vÃ  training model ChatterboxTTS cho tiáº¿ng Viá»‡t tá»« Ä‘áº§u.

## ğŸ¯ Tá»•ng quan

Quy trÃ¬nh nÃ y sáº½:
1. **Táº¡o tokenizer tiáº¿ng Viá»‡t** tá»« text corpus
2. **Merge vá»›i tokenizer gá»‘c** Ä‘á»ƒ giá»¯ kháº£ nÄƒng Ä‘a ngÃ´n ngá»¯
3. **Extend model weights** Ä‘á»ƒ phÃ¹ há»£p vá»›i vocabulary size má»›i
4. **Setup training** vá»›i dataset CSV

## ğŸ“‹ YÃªu cáº§u

### Files cáº§n cÃ³:
```
project/
â”œâ”€â”€ train.csv                 # Training data
â”œâ”€â”€ val.csv                   # Validation data
â”œâ”€â”€ wavs/                     # Audio files directory
â””â”€â”€ chatterbox-project/
    â””â”€â”€ chatterbox_weights/
        â”œâ”€â”€ tokenizer.json    # Original tokenizer
        â”œâ”€â”€ t3_cfg.safetensors # Original T3 model
        â”œâ”€â”€ ve.safetensors    # Voice encoder
        â”œâ”€â”€ s3gen.safetensors # Speech generator
        â””â”€â”€ conds.pt          # Conditions (optional)
```

### Dependencies:
```bash
pip install pandas torch transformers tokenizers safetensors librosa tqdm
```

## ğŸš€ Quick Start

### Cháº¡y setup tá»± Ä‘á»™ng:
```bash
python run_vietnamese_setup.py
```

Script nÃ y sáº½ tá»± Ä‘á»™ng thá»±c hiá»‡n toÃ n bá»™ quy trÃ¬nh vÃ  bÃ¡o cÃ¡o káº¿t quáº£.

## ğŸ“ Manual Setup (tá»«ng bÆ°á»›c)

### BÆ°á»›c 1: Táº¡o text corpus
```bash
# TrÃ­ch xuáº¥t text tá»« CSV files
python extract_vietnamese_text.py \
    --train_csv train.csv \
    --val_csv val.csv \
    --output vietnamese_text_corpus.txt \
    --analyze
```

### BÆ°á»›c 2: Setup tokenizer vÃ  model
```bash
# Táº¡o tokenizer, merge vÃ  extend weights
python setup_vietnamese_training.py \
    --text_file vietnamese_text_corpus.txt \
    --vocab_size 500
```

### BÆ°á»›c 3: Kiá»ƒm tra dataset
```bash
# Validate dataset trÆ°á»›c khi training
python check_dataset.py \
    --train_csv train.csv \
    --val_csv val.csv
```

### BÆ°á»›c 4: Báº¯t Ä‘áº§u training
```bash
# Start training vá»›i config má»›i
python train_vietnamese_csv.py \
    --model_config model_path_vietnamese.json \
    --train_csv train.csv \
    --val_csv val.csv \
    --output_dir checkpoints/vietnamese_tts
```

## ğŸ”§ Chi tiáº¿t cÃ¡c bÆ°á»›c

### 1. Text Corpus Creation
- TrÃ­ch xuáº¥t text tá»« CSV files
- LÃ m sáº¡ch vÃ  chuáº©n hÃ³a text tiáº¿ng Viá»‡t
- PhÃ¢n tÃ­ch thá»‘ng kÃª corpus

### 2. Vietnamese Tokenizer
- Táº¡o BPE tokenizer cho tiáº¿ng Viá»‡t
- Vocab size: 500 tokens (cÃ³ thá»ƒ Ä‘iá»u chá»‰nh)
- TrÃ¡nh overlap vá»›i tokenizer gá»‘c

### 3. Tokenizer Merging
- Merge Vietnamese tokenizer vá»›i original tokenizer
- Giá»¯ nguyÃªn special tokens
- Tá»•ng vocab size: ~1200+ tokens

### 4. Model Weight Extension
- Extend `text_emb.weight` vÃ  `text_head.weight`
- Initialize new tokens vá»›i normal distribution
- Backup original weights

### 5. Training Configuration
- Táº¡o `model_path_vietnamese.json`
- Point Ä‘áº¿n extended model vÃ  merged tokenizer
- Ready cho training

## ğŸ“Š Monitoring Training

### TensorBoard:
```bash
tensorboard --logdir checkpoints/vietnamese_tts/runs
```

### Training logs:
```bash
tail -f training.log
```

## âš™ï¸ Tham sá»‘ tá»‘i Æ°u

### Tokenizer:
- `vocab_size`: 500-1000 (tÃ¹y dataset size)
- `min_frequency`: 2 (filter rare tokens)

### Training:
- `num_train_epochs`: 3-5
- `per_device_train_batch_size`: 4-8 (tÃ¹y GPU)
- `learning_rate`: 3e-5 Ä‘áº¿n 5e-5
- `warmup_steps`: 500-1000

### Model Extension:
- `init_method`: "normal" (recommended)
- `backup_original`: True (safety)

## ğŸ› Troubleshooting

### Lá»—i thÆ°á»ng gáº·p:

1. **"tokenizer.json not found"**
   ```bash
   # Download model weights trÆ°á»›c
   python download_hf_repo.py
   ```

2. **"Vocab size mismatch"**
   ```bash
   # Re-run weight extension
   python setup_vietnamese_training.py --vocab_size 500
   ```

3. **"Out of memory during training"**
   ```bash
   # Giáº£m batch size
   python train_vietnamese_csv.py --per_device_train_batch_size 2
   ```

4. **"Text encoding errors"**
   ```bash
   # Re-extract text corpus
   python extract_vietnamese_text.py --analyze
   ```

### Debug commands:

```bash
# Kiá»ƒm tra tokenizer
python tokenizer_scripts/examine_tokenizer.py tokenizer_vi_merged.json

# Kiá»ƒm tra model weights
python -c "
import torch
from safetensors import safe_open
with safe_open('t3_cfg_vietnamese.safetensors', framework='pt') as f:
    print('text_emb.weight shape:', f.get_tensor('text_emb.weight').shape)
"

# Test tokenization
python -c "
from tokenizers import Tokenizer
tokenizer = Tokenizer.from_file('tokenizer_vi_merged.json')
text = 'Xin chÃ o, tÃ´i lÃ  trá»£ lÃ½ AI'
tokens = tokenizer.encode(text)
print('Tokens:', tokens.tokens)
print('IDs:', tokens.ids)
"
```

## ğŸ“ˆ Expected Results

### Tokenizer Stats:
- Original vocab: ~703 tokens
- Vietnamese vocab: ~500 tokens  
- Merged vocab: ~1200+ tokens
- Special tokens: 4

### Training Progress:
- Initial loss: ~8-10
- Target loss: <2.0
- Training time: 2-4 hours (tÃ¹y dataset vÃ  GPU)

## ğŸ¯ Next Steps

Sau khi training xong:

1. **Test inference:**
   ```bash
   python inference_simple.py --model_config model_path_vietnamese.json
   ```

2. **Evaluate quality:**
   - Nghe audio output
   - Check pronunciation accuracy
   - Test vá»›i different text lengths

3. **Fine-tune parameters:**
   - Adjust learning rate
   - Increase epochs if needed
   - Try different batch sizes

## ğŸ“ Support

Náº¿u gáº·p váº¥n Ä‘á»:
1. Check logs trong `training.log`
2. Verify file paths trong config
3. Test vá»›i smaller dataset trÆ°á»›c
4. Monitor GPU memory usage

ChÃºc báº¡n training thÃ nh cÃ´ng! ğŸµğŸ‡»ğŸ‡³
